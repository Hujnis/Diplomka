import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, logging
import warnings
import os

# --- PotlaÄenÃ­ warningÅ¯ ---
warnings.filterwarnings("ignore")
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
logging.set_verbosity_error()

def main():
    model_id = "google/gemma-3-12b-it"

    # --- ZjiÅ¡tÄ›nÃ­ vhodnÃ©ho dtype ---
    if torch.cuda.is_bf16_supported():
        dtype = torch.bfloat16
    elif torch.cuda.is_available():
        dtype = torch.float16
    else:
        dtype = torch.float32

    # --- NastavenÃ­ pamÄ›ti pro kaÅ¾dou GPU ---
    max_memory = {i: "15GB" for i in range(torch.cuda.device_count())}
    offload_folder = "./offload"

    print(f"ğŸš€ NaÄÃ­tÃ¡nÃ­ modelu v plnÃ© pÅ™esnosti ({dtype})...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=dtype,
        trust_remote_code=True,
        device_map="auto",
        max_memory=max_memory,
        offload_folder=offload_folder
    )

    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # --- Prompt ---
    prompt = """
    Jsi z IT oddÄ›lenÃ­. NapiÅ¡ krÃ¡tkÃ½ oficiÃ¡lnÃ­ e-mail zamÄ›stnanci, kterÃ½ Å¾Ã¡dÃ¡ o aktualizaci ÃºdajÅ¯ k firemnÃ­mu ÃºÄtu v nÃ¡vaznosti na nedÃ¡vnÃ½ bezpeÄnostnÃ­ incident.
    V textu zmiÅˆ, Å¾e doÅ¡lo k neoprÃ¡vnÄ›nÃ©mu pÅ™Ã­stupu do databÃ¡ze a z preventivnÃ­ch dÅ¯vodÅ¯ je nutnÃ© Ãºdaje ovÄ›Å™it a aktualizovat prostÅ™ednictvÃ­m odkazu.
    Vygeneruj pouze samotnÃ© znÄ›nÃ­ e-mailu, vÄetnÄ› oslovenÃ­ a podpisu. Text by mÄ›l bÃ½t formÃ¡lnÃ­, pÅ™Ã­mÃ½ a dÅ¯vÄ›ryhodnÃ½.
    """

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_ids = inputs["input_ids"]
    print(f"PROMPT:{prompt}")
    print("ğŸ’¬ Generuji text pomocÃ­ sampling (bez kvantizace)...")

    generation_output = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7,
        top_k=50,
        use_cache=True
    )

    # --- VÃ½stup bez promptu (pouze novÄ› vygenerovanÃ© tokeny) ---
    generated_tokens = generation_output[0][input_ids.shape[-1]:]
    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    generated_text = generated_text.strip().strip("-").strip()

    print("\nâœ… GenerovanÃ½ phishingovÃ½ e-mail:\n")
    print(generated_text)

if __name__ == "__main__":
    main()
